{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=e2f41e9ffac7cad18f65013082cbc41d2151e9ca40304c68308d89b60bee96ea\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\0a\\9e\\ba\\20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.11.1 bs4-0.0.1 soupsieve-2.3.2.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.delftstack.com/zh-tw/howto/python-pandas/convert-pandas-dataframe-to-dictionary/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def crawling(url): #'https://movies.yahoo.com.tw/chart.html'\n",
    "    response = requests.get(url=url)\n",
    "  \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    rows = soup.find_all('div', class_='tr')\n",
    "    colname = list(rows.pop(0).stripped_strings) #把欄位名稱pop出來，使第一筆資料即為電影資訊\n",
    "    hrefs = soup.select('div.td > a') #電影資訊連結(包括預告片等等)\n",
    "\n",
    "    #擷取電影資訊連結\n",
    "    links = []\n",
    "    for href in hrefs:\n",
    "        a = href.get('href')\n",
    "        if not a.endswith('.html'): #電影資訊頁面 的連結結尾沒有.html\n",
    "            links.append(a) \n",
    "\n",
    "    #將排名、片名、評分、連結置入data(list)\n",
    "    data = []\n",
    "    for i in range(len(rows)):\n",
    "\n",
    "      data.append([i+1])\n",
    "      data_in = False\n",
    "      find_a = rows[i].find('a')\n",
    "      #判斷電影是否有資訊連結\n",
    "      if find_a:\n",
    "          find_href = find_a.get('href')\n",
    "          if find_href:\n",
    "              data_in = True\n",
    "          else:\n",
    "              data[i].append(None)\n",
    "              continue\n",
    "      else:\n",
    "          data[i].append(None)\n",
    "          continue\n",
    "\n",
    "      #若有資訊連結(才可繼續後面的相關資料蒐集)\n",
    "      if data_in == True:\n",
    "      \n",
    "          try:\n",
    "              a = eval(list(rows[i].stripped_strings)[1]) # 因為[1]筆資料可能是上周排序或是片名，所以此處如果eval成功(代表是排序)則擷取[2](片名)\n",
    "              data[i].append(list(rows[i].stripped_strings)[2])\n",
    "          except:\n",
    "              data[i].append(list(rows[i].stripped_strings)[1]) # 若因eval報錯，則[1]就是片名。\n",
    "          data[i].append(list(rows[i].stripped_strings)[-1]) # [-1]為電影評分\n",
    "          data[i].append(find_href) # 電影資訊連結\n",
    "\n",
    "\n",
    "    #將data置入dataframe\n",
    "    data_df = pd.DataFrame(data, columns = [\"本周排名\",\"片名\",\"評分\",\"連結\"])\n",
    "    data_df.dropna(inplace=True) #去除資料不完整的電影資訊\n",
    "    data_df['本周排名'] = [i+1 for i in range(data_df.shape[0])] #重新排名\n",
    "    data_df.reset_index(drop=True,inplace=True)\n",
    "    #data_df\n",
    "\n",
    "    #擷取上映日期、片長、發行公司\n",
    "    date = []  \n",
    "    time = []\n",
    "    firm = []\n",
    "    director = []\n",
    "    intro = []\n",
    "    img = []\n",
    "    time_url = []\n",
    "    poster = []\n",
    "\n",
    "    for url in data_df['連結']:\n",
    "        response = requests.get(url=url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        infos = soup.select(\"div.movie_intro_info_r > span\")\n",
    "        \n",
    "        #擷取上映日期、片長、發行公司\n",
    "        for i in range(3): #0~2 [\"上映日期\", \"片長\",\"發行公司\"]\n",
    "            index_colon = infos[i].string.find(\"：\")\n",
    "            if i == 0: #上映日期\n",
    "                date.append(str(infos[i].string)[index_colon+1:])\n",
    "            elif i == 1: #片長 \n",
    "                time.append(str(infos[i].string)[index_colon+1:])\n",
    "            elif i == 2: #發行公司\n",
    "                firm.append(str(infos[i].string)[index_colon+1:]) \n",
    "\n",
    "      #擷取導演的部分，網站上有兩種存放格式。\n",
    "      #1.單純文字 2.有超連結。因此，需要先從最外層(\"div.movie_intro_list\")抓抓看，若抓不到回傳None，則往<a>尋找(\"div.movie_intro_list > a\")\n",
    "        directors = soup.select(\"span.movie_intro_list\")\n",
    "        # print(directors[0].text.replace(\"導演：\",\"\").strip())\n",
    "        # if (directors[0].string) == None:\n",
    "        #   directors = soup.select(\"div.movie_intro_list > a\")\n",
    "\n",
    "        director.append(directors[0].text.replace(\"導演：\",\"\").strip())\n",
    "\n",
    "        #擷取劇情介紹\n",
    "        infos = soup.select(\"div.gray_infobox_inner > span\")\n",
    "        intro.append(str(infos[0].string).strip()) \n",
    "\n",
    "        #電影海報\n",
    "        infos = soup.select(\"div.movie_intro_foto > img\")\n",
    "        img.append(infos[0]['src'])\n",
    "\n",
    "        #時刻表連結\n",
    "        movie_id = url[-5:] #電影id目前只使用到10000多，若電影編號小於10000或大於99999，則可以擷取\"=\"後面的數字。\n",
    "        time_url.append(f\"https://movies.yahoo.com.tw/movietime_result.html?movie_id={movie_id}\")\n",
    "\n",
    "        #劇照連結\n",
    "        poster.append(f'https://movies.yahoo.com.tw/movieinfo_photos.html/id={movie_id}')\n",
    "      \n",
    "    data_df[\"電影海報\"] = img\n",
    "    data_df[\"上映日期\"] = date\n",
    "    data_df[\"片長\"] = time\n",
    "    data_df[\"發行公司\"] = firm\n",
    "    data_df[\"導演\"] = director\n",
    "    data_df[\"劇情介紹\"] = intro\n",
    "    data_df['時刻表連結'] = time_url\n",
    "    data_df['劇照連結'] = poster\n",
    "\n",
    "    poster_url = []\n",
    "    for i in range(len(data_df['劇照連結'])):\n",
    "        poster_url.append([])\n",
    "        response = requests.get(url=data_df['劇照連結'][i])\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        poster_link = soup.find_all('img')\n",
    "        for k in range(len(poster_link)):\n",
    "            alt = data_df[\"片名\"][i] + \"劇照\"\n",
    "            match = poster_link[k]['alt']\n",
    "            # print(f'match = {match}, alt = {alt}, match == alt = {match[:3] == alt[:3]}')\n",
    "            \n",
    "            # 有些alt的命名很偷懶，會有標點符號全半型的問題 或是 簡寫片名的問題，因此，相對應只比對前三個字元 或 後四個字元(最後兩個字元必為劇照)\n",
    "            if match[:3] == alt[:3] or match[-4:] == alt[-4:]:  \n",
    "                poster_url[i].append(poster_link[k]['data-src']) \n",
    "\n",
    "    data_df['劇照'] = poster_url\n",
    "\n",
    "    return data_df\n",
    "\n",
    "df = crawling(\"https://movies.yahoo.com.tw/chart.html\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'林皓申'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到為甚麼會沒有劇照的問題了，因為全半形冒號的關係\n",
    "df['導演'][10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12b51c3a15c6d04bbe25e0ed0a8589f4bf8f73b3e40dc1f9d202d81fcb7450ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
